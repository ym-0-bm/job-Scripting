{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54757764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de509244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.indeed.com/jobs?q=data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6da7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf44b680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_elems = soup.find_all('li', class_='css-5lfssm')\n",
    "job_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85dc9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_elem in job_elems:\n",
    "    title_elem = job_elem.find('h2', class_='title')\n",
    "    company_elem = job_elem.find('span', class_='company')\n",
    "    location_elem = job_elem.find('div', class_='location')\n",
    "    if None in (title_elem, company_elem, location_elem):\n",
    "        continue\n",
    "    print(title_elem.text.strip())\n",
    "    print(company_elem.text.strip())\n",
    "    print(location_elem.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19f530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la recherche Indeed\n",
    "url = \"https://www.indeed.com/jobs?q=developer&l=&vjk=3dbc51534ebe66cd\"\n",
    "\n",
    "# Obtenir le contenu HTML de la page\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Analyser le contenu HTML avec Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# Extraire les offres d'emploi\n",
    "offres_emploi = soup.find_all('div', class_='jobsearch-HeaderContainer')\n",
    "\n",
    "# Parcourir chaque offre d'emploi\n",
    "for offre in offres_emploi:\n",
    "    # Titre de l'offre\n",
    "    titre = offre.find('a', class_='jobtitle').text.strip()\n",
    "\n",
    "    # Entreprise\n",
    "    entreprise = offre.find('span', class_='company').text.strip()\n",
    "\n",
    "    # Lieu\n",
    "    lieu = offre.find('span', class_='location').text.strip()\n",
    "\n",
    "    # Description\n",
    "    description = offre.find('div', class_='jobdesc').text.strip()\n",
    "\n",
    "    # URL de l'offre\n",
    "    lien_offre = offre.find('a', class_='jobtitle')['href']\n",
    "\n",
    "    # Afficher les informations de l'offre\n",
    "    print(f\"Titre: {titre}\")\n",
    "    print(f\"Entreprise: {entreprise}\")\n",
    "    print(f\"Lieu: {lieu}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"URL: {lien_offre}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aec1290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Skill: data\n",
      "Enter the location: abidjan\n",
      "Enter the #pages to scrape: 1\n",
      "\n",
      "Scraping in progress...\n",
      "\n",
      "Jobs data written to <Data_Abidjan_Jobs.csv> successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    \"User-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
    "\n",
    "# Skills and Place of Work\n",
    "skill = input('Enter your Skill: ').strip()\n",
    "place = input('Enter the location: ').strip()\n",
    "no_of_pages = int(input('Enter the #pages to scrape: '))\n",
    "\n",
    "\n",
    "# Creating the Main Directory\n",
    "main_dir = os.getcwd() + '\\\\'\n",
    "if not os.path.exists(main_dir):\n",
    "    os.mkdir(main_dir)\n",
    "    print('Base Directory Created Successfully.')\n",
    "\n",
    "\n",
    "# Name of the CSV File\n",
    "file_name = skill.title() + '_' + place.title() + '_Jobs.csv'\n",
    "# Path of the CSV File\n",
    "file_path = main_dir + file_name\n",
    "\n",
    "# Writing to the CSV File\n",
    "with open(file_path, mode='w') as file:\n",
    "    writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "    # Adding the Column Names to the CSV File\n",
    "    writer.writerow(\n",
    "        ['JOB_NAME', 'COMPANY', 'LOCATION', 'POSTED', 'APPLY_LINK'])\n",
    "\n",
    "    # Requesting and getting the webpage using requests\n",
    "    print(f'\\nScraping in progress...\\n')\n",
    "    for page in range(no_of_pages):\n",
    "        url = 'https://www.indeed.com/jobs?q=' + skill + \\\n",
    "            '&l=' + place + '&start=' + str(page * 10)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        html = response.text\n",
    "\n",
    "        # Scrapping the Web\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        base_url = 'https://in.indeed.com/viewjob?jk='\n",
    "        d = soup.find('div', attrs={'id': 'mosaic-provider-jobcards'})\n",
    "\n",
    "        jobs = soup.find_all('a', class_='tapItem')\n",
    "\n",
    "        for job in jobs:\n",
    "            job_id = job['id'].split('_')[-1]\n",
    "            job_title = job.find('span', title=True).text.strip()\n",
    "            company = job.find('span', class_='companyName').text.strip()\n",
    "            location = job.find('div', class_='companyLocation').text.strip()\n",
    "            posted = job.find('span', class_='date').text.strip()\n",
    "            job_link = base_url + job_id\n",
    "            #print([job_title, company, location, posted, job_link])\n",
    "\n",
    "            # Writing to CSV File\n",
    "            writer.writerow(\n",
    "                [job_title, company, location.title(), posted, job_link])\n",
    "\n",
    "print(f'Jobs data written to <{file_name}> successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25812801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_NAME</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>POSTED</th>\n",
       "      <th>APPLY_LINK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [JOB_NAME, COMPANY, LOCATION, POSTED, APPLY_LINK]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data_Abidjan_Jobs.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382f6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.com/jobs?q=data'\n",
    "# + skill + \\\n",
    "#    '&l=' + place + '&start=' + str(page * 10)\n",
    "response = requests.get(url, headers=headers)\n",
    "html = response.text\n",
    "\n",
    "# Scrapping the Web\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "base_url = 'https://in.indeed.com/viewjob?jk='\n",
    "d = soup.find('div', attrs={'id': 'mosaic-provider-jobcards'})\n",
    "\n",
    "jobs = soup.find_all('a', class_='tapItem')\n",
    "\n",
    "for job in jobs:\n",
    "    job_id = job['id'].split('_')[-1]\n",
    "    job_title = job.find('span', title=True).text.strip()\n",
    "    company = job.find('span', class_='companyName').text.strip()\n",
    "    location = job.find('div', class_='companyLocation').text.strip()\n",
    "    posted = job.find('span', class_='date').text.strip()\n",
    "    job_link = base_url + job_id\n",
    "    #print([job_title, company, location, posted, job_link])\n",
    "    # Afficher les informations de l'offre\n",
    "    print(f\"Titre: {job_title}\")\n",
    "    print(f\"Entreprise: {company}\")\n",
    "    print(f\"Lieu: {location}\")\n",
    "    print(f\"Description: {posted}\")\n",
    "    print(f\"URL: {job_link}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "web scraping functionality from www.indeed.com (USA)\n",
    "\"\"\"\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "source = \"indeed.com\"\n",
    "cookies = {'aep_usuc_f': 'region=US&site=glo&b_locale=en_US&c_tp=USD'}\n",
    "\n",
    "\n",
    "def get_url(position):\n",
    "    \"\"\"\n",
    "    Generate URL from position and company type: recruiter or direct employer; education level\n",
    "    \"\"\"\n",
    "    url = f\"https://indeed.com/jobs?q={position}\"\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_job_date(card):\n",
    "    \"\"\"\n",
    "     extracts date from the job post record\n",
    "\n",
    "    :param card:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    post_str = card.find('span', {'class': 'date'}).text  # text from the footer: days ago was posted\n",
    "    post_days = re.findall(r'\\d+', post_str)  # extracting number of days from posted_str\n",
    "\n",
    "    if post_days:\n",
    "        # calculated date of job posting if days are mentioned\n",
    "        job_date = (datetime.now() - timedelta(days=int(post_days[0]))).strftime(\"%d/%m/%Y\")\n",
    "    else:\n",
    "        job_date = datetime.now().strftime(\"%d/%m/%Y\")  # if days are not mentioned - using today\n",
    "\n",
    "    return job_date\n",
    "\n",
    "\n",
    "def get_job_salaries(card):\n",
    "    \"\"\"\n",
    "    extracts salaries\n",
    "    :param card:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        salary_str = card.find('div', 'metadata salary-snippet-container').text\n",
    "        salaries = re.findall(r\"\\b(\\w+[.]\\w+)\", salary_str)\n",
    "\n",
    "    except AttributeError:\n",
    "        salaries = []\n",
    "\n",
    "    return salaries\n",
    "\n",
    "\n",
    "def get_record(card):\n",
    "    \"\"\"\n",
    "    Extract job data from a single record\n",
    "    \"\"\"\n",
    "    span_tag = card.h2.a.span\n",
    "    a_tag = card.h2.a\n",
    "\n",
    "    job_id = a_tag.get(\"data-jk\")  # unique job id\n",
    "    job_title = span_tag.get(\"title\")  # job title\n",
    "    job_url = 'https://www.indeed.com' + a_tag.get('href')  # job url\n",
    "    company_name = card.find('span', {'class': 'companyName'}).text  # company name\n",
    "    job_loc = card.find('div', {'class': 'companyLocation'}).text  # job location\n",
    "    job_summary = card.find('div', {'class': 'job-snippet'}).text.strip()  # job description\n",
    "    job_date = get_job_date(card)  # job posting date\n",
    "    job_salary = get_job_salaries(card)  # job salaries if any\n",
    "\n",
    "    record = (job_id, job_title, job_date, job_loc, job_summary, job_salary, job_url, company_name)\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def get_jobs(position):\n",
    "    \"\"\"\n",
    "    creates a DataFrame with all records (scraped jobs), scraping from all pages\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = get_url(position)\n",
    "    records = []\n",
    "\n",
    "    # extract the job data\n",
    "\n",
    "    while True:\n",
    "\n",
    "        response = \"\"\n",
    "        while response == \"\":\n",
    "            try:\n",
    "                response = requests.get(url=url, cookies=cookies)\n",
    "                break\n",
    "            except ConnectionError:\n",
    "                print(\"Connection refused by the server..\")\n",
    "                print(\"Let me sleep for 5 seconds\")\n",
    "                print(\"ZZzzzz...\")\n",
    "                time.sleep(5)\n",
    "                print(\"Was a nice sleep, now let me continue...\")\n",
    "                continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        cards = soup.find_all('div', 'job_seen_beacon')\n",
    "\n",
    "        for card in cards:\n",
    "            record = get_record(card)\n",
    "            records.append(record)\n",
    "\n",
    "        time.sleep(3)  # making a pause before moving to the next page\n",
    "\n",
    "        # moving to the next page - > assigning a new url\n",
    "        try:\n",
    "            url = 'https://indeed.com/' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    # save the data as DF\n",
    "    columns = ['job_id',\n",
    "               'job_title',\n",
    "               'job_date',\n",
    "               'job_loc',\n",
    "               'job_summary',\n",
    "               'job_salary',\n",
    "               'job_url',\n",
    "               'company_name']\n",
    "    df = pd.DataFrame(data=records, columns=columns)\n",
    "\n",
    "    # adding to DF columns with search parameters\n",
    "    search_time = datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "\n",
    "    df[\"search_time\"] = search_time\n",
    "    df[\"search_position\"] = position\n",
    "    df[\"source\"] = source\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2957bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "web scraping functionality from www.indeed.com (USA)\n",
    "\"\"\"\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "source = \"indeed.com\"\n",
    "cookies = {'aep_usuc_f': 'region=US&site=glo&b_locale=en_US&c_tp=USD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c42d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://indeed.com/jobs?q=data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02cf41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
