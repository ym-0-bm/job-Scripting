{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c63152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "web scraping functionality from www.indeed.com (USA)\n",
    "\"\"\"\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "source = \"indeed.com\"\n",
    "cookies = {'aep_usuc_f': 'region=US&site=glo&b_locale=en_US&c_tp=USD'}\n",
    "\n",
    "\n",
    "def get_url(position):\n",
    "    \"\"\"\n",
    "    Generate URL from position and company type: recruiter or direct employer; education level\n",
    "    \"\"\"\n",
    "    url = f\"https://indeed.com/jobs?q={position}\"\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_job_date(card):\n",
    "    \"\"\"\n",
    "     extracts date from the job post record\n",
    "\n",
    "    :param card:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    post_str = card.find('span', {'class': 'date'}).text  # text from the footer: days ago was posted\n",
    "    post_days = re.findall(r'\\d+', post_str)  # extracting number of days from posted_str\n",
    "\n",
    "    if post_days:\n",
    "        # calculated date of job posting if days are mentioned\n",
    "        job_date = (datetime.now() - timedelta(days=int(post_days[0]))).strftime(\"%d/%m/%Y\")\n",
    "    else:\n",
    "        job_date = datetime.now().strftime(\"%d/%m/%Y\")  # if days are not mentioned - using today\n",
    "\n",
    "    return job_date\n",
    "\n",
    "\n",
    "def get_job_salaries(card):\n",
    "    \"\"\"\n",
    "    extracts salaries\n",
    "    :param card:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        salary_str = card.find('div', 'metadata salary-snippet-container').text\n",
    "        salaries = re.findall(r\"\\b(\\w+[.]\\w+)\", salary_str)\n",
    "\n",
    "    except AttributeError:\n",
    "        salaries = []\n",
    "\n",
    "    return salaries\n",
    "\n",
    "\n",
    "def get_record(card):\n",
    "    \"\"\"\n",
    "    Extract job data from a single record\n",
    "    \"\"\"\n",
    "    span_tag = card.h2.a.span\n",
    "    a_tag = card.h2.a\n",
    "\n",
    "    job_id = a_tag.get(\"data-jk\")  # unique job id\n",
    "    job_title = span_tag.get(\"title\")  # job title\n",
    "    job_url = 'https://www.indeed.com' + a_tag.get('href')  # job url\n",
    "    company_name = card.find('span', {'class': 'companyName'}).text  # company name\n",
    "    job_loc = card.find('div', {'class': 'companyLocation'}).text  # job location\n",
    "    job_summary = card.find('div', {'class': 'job-snippet'}).text.strip()  # job description\n",
    "    job_date = get_job_date(card)  # job posting date\n",
    "    job_salary = get_job_salaries(card)  # job salaries if any\n",
    "\n",
    "    record = (job_id, job_title, job_date, job_loc, job_summary, job_salary, job_url, company_name)\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def get_jobs(position):\n",
    "    \"\"\"\n",
    "    creates a DataFrame with all records (scraped jobs), scraping from all pages\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = get_url(position)\n",
    "    records = []\n",
    "\n",
    "    # extract the job data\n",
    "\n",
    "    while True:\n",
    "\n",
    "        response = \"\"\n",
    "        while response == \"\":\n",
    "            try:\n",
    "                response = requests.get(url=url, cookies=cookies)\n",
    "                break\n",
    "            except ConnectionError:\n",
    "                print(\"Connection refused by the server..\")\n",
    "                print(\"Let me sleep for 5 seconds\")\n",
    "                print(\"ZZzzzz...\")\n",
    "                time.sleep(5)\n",
    "                print(\"Was a nice sleep, now let me continue...\")\n",
    "                continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        cards = soup.find_all('div', 'job_seen_beacon')\n",
    "\n",
    "        for card in cards:\n",
    "            record = get_record(card)\n",
    "            records.append(record)\n",
    "\n",
    "        time.sleep(3)  # making a pause before moving to the next page\n",
    "\n",
    "        # moving to the next page - > assigning a new url\n",
    "        try:\n",
    "            url = 'https://indeed.com/' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    # save the data as DF\n",
    "    columns = ['job_id',\n",
    "               'job_title',\n",
    "               'job_date',\n",
    "               'job_loc',\n",
    "               'job_summary',\n",
    "               'job_salary',\n",
    "               'job_url',\n",
    "               'company_name']\n",
    "    df = pd.DataFrame(data=records, columns=columns)\n",
    "\n",
    "    # adding to DF columns with search parameters\n",
    "    search_time = datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "\n",
    "    df[\"search_time\"] = search_time\n",
    "    df[\"search_position\"] = position\n",
    "    df[\"source\"] = source\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e621a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functionality for transforming scraping results into a data-dump ready to be uploaded to a DB or saved to .csv\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataDump:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def merge(self, df):\n",
    "        \"\"\"\n",
    "        Concatenates new df to the datadump\n",
    "\n",
    "        :param df: pd.DataFrame\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if len(self.df) != 0:\n",
    "            self.df = pd.concat(objs=[self.df, df])\n",
    "        else:\n",
    "            self.df = df\n",
    "\n",
    "    def remove_duplicates(self, field):\n",
    "        \"\"\"\n",
    "        Removes duplicates from a df by the 'JobID' field.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.df.sort_values(by=field, inplace=True)\n",
    "        self.df.drop_duplicates(subset=[field], keep=\"first\", inplace=True)\n",
    "\n",
    "    def format_salaries(self, field):\n",
    "        self.df[field] = self.df[field].astype(\"string\")\n",
    "\n",
    "    def save_to_csv(self, path=None):\n",
    "        \"\"\"\n",
    "        Saves a DataFrame into CSV file\n",
    "        :param path: string\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        suffix = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "        file_name = f\"datadump_{suffix}.csv\"\n",
    "        self.df.to_csv(path + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b3c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "module with functionality of the logging the results of the program run.\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    return datetime.now().strftime('%d-%m-%Y, %H:%M:%S')\n",
    "\n",
    "\n",
    "def logging(time, message, logs_lst):\n",
    "    log = (time, message)\n",
    "    logs_lst.append(log)\n",
    "    print(time + ': ' + message)\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.logs_lst = []\n",
    "\n",
    "    def start_session(self):\n",
    "        time = get_time()\n",
    "        message = \"Session starts\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def end_session(self):\n",
    "        time = get_time()\n",
    "        message = \"Session ends.\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def start_scraping(self, position):\n",
    "        time = get_time()\n",
    "        message = f\"Scraping attempt with the key word: '{position}'... \"\n",
    "\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def scraping_result(self, df):\n",
    "        time = get_time()\n",
    "        message = f\"....successfully implemented. Number of records found: {len(df)}\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def scraping_result_final(self, df):\n",
    "        time = get_time()\n",
    "        message = f\"Full scraping is executed. The raw data dump file contains {len(df)} records.\\n\" \\\n",
    "                  f\"Number of unique records is {df['job_id'].nunique()}.\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def data_formatted(self, df):\n",
    "        time = get_time()\n",
    "        message = f\"Data dump is formatted:\\n {df.info()}\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def error_occurs(self, error):\n",
    "        time = get_time()\n",
    "        message = f\"{error} occurs\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def save_to_txt(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77cb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import indeed_com_scraper as scraper\n",
    "# from dumping import DataDump\n",
    "# from logger import Logger\n",
    "\n",
    "\n",
    "class ScrapingSession:\n",
    "    def __init__(self, positions):\n",
    "        self.positions = positions\n",
    "\n",
    "    def run(self):\n",
    "        data_dump = DataDump()\n",
    "        logger = Logger()\n",
    "\n",
    "        # scraping -> creates a compiled data dump from different search parameters\n",
    "        logger.start_session()  # logging start of a session\n",
    "\n",
    "        for position in self.positions:\n",
    "            logger.start_scraping(position)  # logging each scraping attempt\n",
    "            try:\n",
    "                df = scraper.get_jobs(position)\n",
    "            except Exception as inst:\n",
    "                logger.error_occurs(inst)\n",
    "            else:\n",
    "                logger.scraping_result(df)  # logging scraping results\n",
    "                data_dump.merge(df)  # merging all values to the data_dump\n",
    "            finally:\n",
    "                time.sleep(5)  # wait for 5 seconds to avoid a block\n",
    "                continue\n",
    "\n",
    "        # logging scraping results\n",
    "        logger.scraping_result_final(data_dump.df)\n",
    "\n",
    "        # data cleansing / formatting\n",
    "        data_dump.remove_duplicates(field=\"job_id\")  # removing duplicates\n",
    "\n",
    "        # logging formatted data dump information\n",
    "        logger.data_formatted(data_dump.df)\n",
    "\n",
    "        # saving as a csv file\n",
    "        data_dump.save_to_csv(path=\"data_dumps/\")\n",
    "\n",
    "        logger.end_session()\n",
    "        logger.save_to_txt()  # function is yet to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66665fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all required arguments for the program: parameters for the search\n",
    "\"\"\"\n",
    "\n",
    "# list of any chosen key-words\n",
    "positions = [\"auditor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f35785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-06-2024, 18:40:37: Session starts\n",
      "04-06-2024, 18:40:37: Scraping attempt with the key word: 'auditor'... \n",
      "04-06-2024, 18:40:37: name 'scraper' is not defined occurs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'job_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m session \u001b[38;5;241m=\u001b[39m ScrapingSession(positions)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m     session\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mScrapingSession.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# logging scraping results\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m logger\u001b[38;5;241m.\u001b[39mscraping_result_final(data_dump\u001b[38;5;241m.\u001b[39mdf)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# data cleansing / formatting\u001b[39;00m\n\u001b[0;32m     35\u001b[0m data_dump\u001b[38;5;241m.\u001b[39mremove_duplicates(field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# removing duplicates\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mLogger.scraping_result_final\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscraping_result_final\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[0;32m     43\u001b[0m     time \u001b[38;5;241m=\u001b[39m get_time()\n\u001b[0;32m     44\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull scraping is executed. The raw data dump file contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m---> 45\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of unique records is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     logging(time, message, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs_lst)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'job_id'"
     ]
    }
   ],
   "source": [
    "# from parameters import positions\n",
    "# from main import ScrapingSession\n",
    "\n",
    "session = ScrapingSession(positions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    session.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
