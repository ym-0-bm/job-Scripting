{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c63152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "web scraping functionality from www.indeed.com (USA)\n",
    "\"\"\"\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "source = \"indeed.com\"\n",
    "cookies = {'aep_usuc_f': 'region=US&site=glo&b_locale=en_US&c_tp=USD'}\n",
    "\n",
    "\n",
    "def get_url(position):\n",
    "    \"\"\"\n",
    "    Generate URL from position and company type: recruiter or direct employer; education level\n",
    "    \"\"\"\n",
    "    url = f\"https://indeed.com/jobs?q={position}\"\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_job_date(card):\n",
    "    \"\"\"\n",
    "     extracts date from the job post record\n",
    "\n",
    "    :param card:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    post_str = card.find('span', {'class': 'date'}).text  # text from the footer: days ago was posted\n",
    "    post_days = re.findall(r'\\d+', post_str)  # extracting number of days from posted_str\n",
    "\n",
    "    if post_days:\n",
    "        # calculated date of job posting if days are mentioned\n",
    "        job_date = (datetime.now() - timedelta(days=int(post_days[0]))).strftime(\"%d/%m/%Y\")\n",
    "    else:\n",
    "        job_date = datetime.now().strftime(\"%d/%m/%Y\")  # if days are not mentioned - using today\n",
    "\n",
    "    return job_date\n",
    "\n",
    "\n",
    "def get_job_salaries(card):\n",
    "    \"\"\"\n",
    "    extracts salaries\n",
    "    :param card:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        salary_str = card.find('div', 'metadata salary-snippet-container').text\n",
    "        salaries = re.findall(r\"\\b(\\w+[.]\\w+)\", salary_str)\n",
    "\n",
    "    except AttributeError:\n",
    "        salaries = []\n",
    "\n",
    "    return salaries\n",
    "\n",
    "\n",
    "def get_record(card):\n",
    "    \"\"\"\n",
    "    Extract job data from a single record\n",
    "    \"\"\"\n",
    "    span_tag = card.h2.a.span\n",
    "    a_tag = card.h2.a\n",
    "\n",
    "    job_id = a_tag.get(\"data-jk\")  # unique job id\n",
    "    job_title = span_tag.get(\"title\")  # job title\n",
    "    job_url = 'https://www.indeed.com' + a_tag.get('href')  # job url\n",
    "    company_name = card.find('span', {'class': 'companyName'}).text  # company name\n",
    "    job_loc = card.find('div', {'class': 'companyLocation'}).text  # job location\n",
    "    job_summary = card.find('div', {'class': 'job-snippet'}).text.strip()  # job description\n",
    "    job_date = get_job_date(card)  # job posting date\n",
    "    job_salary = get_job_salaries(card)  # job salaries if any\n",
    "\n",
    "    record = (job_id, job_title, job_date, job_loc, job_summary, job_salary, job_url, company_name)\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def get_jobs(position):\n",
    "    \"\"\"\n",
    "    creates a DataFrame with all records (scraped jobs), scraping from all pages\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = get_url(position)\n",
    "    records = []\n",
    "\n",
    "    # extract the job data\n",
    "\n",
    "    while True:\n",
    "\n",
    "        response = \"\"\n",
    "        while response == \"\":\n",
    "            try:\n",
    "                response = requests.get(url=url, cookies=cookies)\n",
    "                break\n",
    "            except ConnectionError:\n",
    "                print(\"Connection refused by the server..\")\n",
    "                print(\"Let me sleep for 5 seconds\")\n",
    "                print(\"ZZzzzz...\")\n",
    "                time.sleep(5)\n",
    "                print(\"Was a nice sleep, now let me continue...\")\n",
    "                continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        cards = soup.find_all('div', 'job_seen_beacon')\n",
    "\n",
    "        for card in cards:\n",
    "            record = get_record(card)\n",
    "            records.append(record)\n",
    "\n",
    "        time.sleep(3)  # making a pause before moving to the next page\n",
    "\n",
    "        # moving to the next page - > assigning a new url\n",
    "        try:\n",
    "            url = 'https://indeed.com/' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    # save the data as DF\n",
    "    columns = ['job_id',\n",
    "               'job_title',\n",
    "               'job_date',\n",
    "               'job_loc',\n",
    "               'job_summary',\n",
    "               'job_salary',\n",
    "               'job_url',\n",
    "               'company_name']\n",
    "    df = pd.DataFrame(data=records, columns=columns)\n",
    "\n",
    "    # adding to DF columns with search parameters\n",
    "    search_time = datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "\n",
    "    df[\"search_time\"] = search_time\n",
    "    df[\"search_position\"] = position\n",
    "    df[\"source\"] = source\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e621a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functionality for transforming scraping results into a data-dump ready to be uploaded to a DB or saved to .csv\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataDump:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def merge(self, df):\n",
    "        \"\"\"\n",
    "        Concatenates new df to the datadump\n",
    "\n",
    "        :param df: pd.DataFrame\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if len(self.df) != 0:\n",
    "            self.df = pd.concat(objs=[self.df, df])\n",
    "        else:\n",
    "            self.df = df\n",
    "\n",
    "    def remove_duplicates(self, field):\n",
    "        \"\"\"\n",
    "        Removes duplicates from a df by the 'JobID' field.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.df.sort_values(by=field, inplace=True)\n",
    "        self.df.drop_duplicates(subset=[field], keep=\"first\", inplace=True)\n",
    "\n",
    "    def format_salaries(self, field):\n",
    "        self.df[field] = self.df[field].astype(\"string\")\n",
    "\n",
    "    def save_to_csv(self, path=None):\n",
    "        \"\"\"\n",
    "        Saves a DataFrame into CSV file\n",
    "        :param path: string\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        suffix = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "        file_name = f\"datadump_{suffix}.csv\"\n",
    "        self.df.to_csv(path + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b3c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "module with functionality of the logging the results of the program run.\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    return datetime.now().strftime('%d-%m-%Y, %H:%M:%S')\n",
    "\n",
    "\n",
    "def logging(time, message, logs_lst):\n",
    "    log = (time, message)\n",
    "    logs_lst.append(log)\n",
    "    print(time + ': ' + message)\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.logs_lst = []\n",
    "\n",
    "    def start_session(self):\n",
    "        time = get_time()\n",
    "        message = \"Session starts\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def end_session(self):\n",
    "        time = get_time()\n",
    "        message = \"Session ends.\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def start_scraping(self, position):\n",
    "        time = get_time()\n",
    "        message = f\"Scraping attempt with the key word: '{position}'... \"\n",
    "\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def scraping_result(self, df):\n",
    "        time = get_time()\n",
    "        message = f\"....successfully implemented. Number of records found: {len(df)}\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def scraping_result_final(self, df):\n",
    "        time = get_time()\n",
    "        message = f\"Full scraping is executed. The raw data dump file contains {len(df)} records.\\n\" \\\n",
    "                  f\"Number of unique records is {df['job_id'].nunique()}.\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def data_formatted(self, df):\n",
    "        time = get_time()\n",
    "        message = f\"Data dump is formatted:\\n {df.info()}\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def error_occurs(self, error):\n",
    "        time = get_time()\n",
    "        message = f\"{error} occurs\"\n",
    "        logging(time, message, self.logs_lst)\n",
    "\n",
    "    def save_to_txt(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed216a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77cb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import indeed_com_scraper as scraper\n",
    "# from dumping import DataDump\n",
    "# from logger import Logger\n",
    "\n",
    "\n",
    "class ScrapingSession:\n",
    "    def __init__(self, positions):\n",
    "        self.positions = positions\n",
    "\n",
    "    def run(self):\n",
    "        data_dump = DataDump()\n",
    "        logger = Logger()\n",
    "\n",
    "        # scraping -> creates a compiled data dump from different search parameters\n",
    "        logger.start_session()  # logging start of a session\n",
    "\n",
    "        for position in self.positions:\n",
    "            logger.start_scraping(position)  # logging each scraping attempt\n",
    "            try:\n",
    "                df = scraper.get_jobs(position)\n",
    "            except Exception as inst:\n",
    "                logger.error_occurs(inst)\n",
    "            else:\n",
    "                logger.scraping_result(df)  # logging scraping results\n",
    "                data_dump.merge(df)  # merging all values to the data_dump\n",
    "            finally:\n",
    "                time.sleep(5)  # wait for 5 seconds to avoid a block\n",
    "                continue\n",
    "\n",
    "        # logging scraping results\n",
    "        logger.scraping_result_final(data_dump.df)\n",
    "\n",
    "        # data cleansing / formatting\n",
    "        data_dump.remove_duplicates(field=\"job_id\")  # removing duplicates\n",
    "\n",
    "        # logging formatted data dump information\n",
    "        logger.data_formatted(data_dump.df)\n",
    "\n",
    "        # saving as a csv file\n",
    "        data_dump.save_to_csv(path=\"data_dumps/\")\n",
    "\n",
    "        logger.end_session()\n",
    "        logger.save_to_txt()  # function is yet to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66665fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all required arguments for the program: parameters for the search\n",
    "\"\"\"\n",
    "\n",
    "# list of any chosen key-words\n",
    "positions = [\"auditor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f35785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-06-2024, 18:40:37: Session starts\n",
      "04-06-2024, 18:40:37: Scraping attempt with the key word: 'auditor'... \n",
      "04-06-2024, 18:40:37: name 'scraper' is not defined occurs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'job_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m session \u001b[38;5;241m=\u001b[39m ScrapingSession(positions)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m     session\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mScrapingSession.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# logging scraping results\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m logger\u001b[38;5;241m.\u001b[39mscraping_result_final(data_dump\u001b[38;5;241m.\u001b[39mdf)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# data cleansing / formatting\u001b[39;00m\n\u001b[0;32m     35\u001b[0m data_dump\u001b[38;5;241m.\u001b[39mremove_duplicates(field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# removing duplicates\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mLogger.scraping_result_final\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscraping_result_final\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[0;32m     43\u001b[0m     time \u001b[38;5;241m=\u001b[39m get_time()\n\u001b[0;32m     44\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull scraping is executed. The raw data dump file contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m---> 45\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of unique records is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     logging(time, message, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs_lst)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'job_id'"
     ]
    }
   ],
   "source": [
    "# from parameters import positions\n",
    "# from main import ScrapingSession\n",
    "\n",
    "session = ScrapingSession(positions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a892f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module \n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "\n",
    "# user define function \n",
    "# Scrape the data \n",
    "# and get in string \n",
    "def getdata(url): \n",
    "    r = requests.get(url) \n",
    "    return r.text \n",
    "\n",
    "# Get Html code using parse \n",
    "def html_code(url): \n",
    "\n",
    "\t# pass the url \n",
    "\t# into getdata function \n",
    "\thtmldata = getdata(url) \n",
    "\tsoup = BeautifulSoup(htmldata, 'html.parser') \n",
    "\n",
    "\t# return html code \n",
    "\treturn(soup) \n",
    "\n",
    "# filter job data using \n",
    "# find_all function \n",
    "def job_data(soup): \n",
    "\t\n",
    "\t# find the Html tag \n",
    "\t# with find() \n",
    "\t# and convert into string \n",
    "\tdata_str = \"\" \n",
    "\tfor item in soup.find_all(\"a\", class_=\"jcs-JobTitle css-jspxzf eu4oa1w0\"): \n",
    "\t\tdata_str = data_str + item.get_text() \n",
    "\tresult_1 = data_str.split(\"\\n\") \n",
    "\treturn(result_1) \n",
    "\n",
    "# filter company_data using \n",
    "# find_all function \n",
    "\n",
    "\n",
    "def company_data(soup): \n",
    "\n",
    "\t# find the Html tag \n",
    "\t# with find() \n",
    "\t# and convert into string \n",
    "\tdata_str = \"\" \n",
    "\tresult = \"\" \n",
    "\tfor item in soup.find_all(\"div\", class_=\"sjcl\"): \n",
    "\t\tdata_str = data_str + item.get_text() \n",
    "\tresult_1 = data_str.split(\"\\n\") \n",
    "\n",
    "\tres = [] \n",
    "\tfor i in range(1, len(result_1)): \n",
    "\t\tif len(result_1[i]) > 1: \n",
    "\t\t\tres.append(result_1[i]) \n",
    "\treturn(res) \n",
    "\n",
    "\n",
    "# driver nodes/main function \n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\t# Data for URL \n",
    "\tjob = \"data+science+internship\"\n",
    "\tLocation = \"Noida%2C+Uttar+Pradesh\"\n",
    "\turl = \"https://in.indeed.com/jobs?q=\"+job+\"&l=\"+Location \n",
    "\n",
    "\t# Pass this URL into the soup \n",
    "\t# which will return \n",
    "\t# html string \n",
    "\tsoup = html_code(url) \n",
    "\n",
    "\t# call job and company data \n",
    "\t# and store into it var \n",
    "\tjob_res = job_data(soup) \n",
    "\tcom_res = company_data(soup) \n",
    "\n",
    "\t# Traverse the both data \n",
    "\ttemp = 0\n",
    "\tfor i in range(1, len(job_res)): \n",
    "\t\tj = temp \n",
    "\t\tfor j in range(temp, 2+temp): \n",
    "\t\t\tprint(\"Company Name and Address : \" + com_res[j]) \n",
    "\n",
    "\t\ttemp = j \n",
    "\t\tprint(\"Job : \" + job_res[i]) \n",
    "\t\tprint(\"-----------------------------\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083638fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<!--[if lt IE 7]> <html class=\"no-js ie6 oldie\" lang=\"en-US\"> <![endif]-->\n",
       "<!--[if IE 7]>    <html class=\"no-js ie7 oldie\" lang=\"en-US\"> <![endif]-->\n",
       "<!--[if IE 8]>    <html class=\"no-js ie8 oldie\" lang=\"en-US\"> <![endif]-->\n",
       "<!--[if gt IE 8]><!--> <html class=\"no-js\" lang=\"en-US\"> <!--<![endif]-->\n",
       "<head>\n",
       "<title>Attention Required! | Cloudflare</title>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
       "<meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
       "<meta content=\"noindex, nofollow\" name=\"robots\"/>\n",
       "<meta content=\"width=device-width,initial-scale=1\" name=\"viewport\"/>\n",
       "<link href=\"/cdn-cgi/styles/cf.errors.css\" id=\"cf_styles-css\" rel=\"stylesheet\"/>\n",
       "<!--[if lt IE 9]><link rel=\"stylesheet\" id='cf_styles-ie-css' href=\"/cdn-cgi/styles/cf.errors.ie.css\" /><![endif]-->\n",
       "<style>body{margin:0;padding:0}</style>\n",
       "<!--[if gte IE 10]><!-->\n",
       "<script>\n",
       "  if (!navigator.cookieEnabled) {\n",
       "    window.addEventListener('DOMContentLoaded', function () {\n",
       "      var cookieEl = document.getElementById('cookie-alert');\n",
       "      cookieEl.style.display = 'block';\n",
       "    })\n",
       "  }\n",
       "</script>\n",
       "<!--<![endif]-->\n",
       "</head>\n",
       "<body>\n",
       "<div id=\"cf-wrapper\">\n",
       "<div class=\"cf-alert cf-alert-error cf-cookie-error\" data-translate=\"enable_cookies\" id=\"cookie-alert\">Please enable cookies.</div>\n",
       "<div class=\"cf-error-details-wrapper\" id=\"cf-error-details\">\n",
       "<div class=\"cf-wrapper cf-header cf-error-overview\">\n",
       "<h1 data-translate=\"block_headline\">Sorry, you have been blocked</h1>\n",
       "<h2 class=\"cf-subheadline\"><span data-translate=\"unable_to_access\">You are unable to access</span> indeed.com</h2>\n",
       "</div><!-- /.header -->\n",
       "<div class=\"cf-section cf-highlight\">\n",
       "<div class=\"cf-wrapper\">\n",
       "<div class=\"cf-screenshot-container cf-screenshot-full\">\n",
       "<span class=\"cf-no-screenshot error\"></span>\n",
       "</div>\n",
       "</div>\n",
       "</div><!-- /.captcha-container -->\n",
       "<div class=\"cf-section cf-wrapper\">\n",
       "<div class=\"cf-columns two\">\n",
       "<div class=\"cf-column\">\n",
       "<h2 data-translate=\"blocked_why_headline\">Why have I been blocked?</h2>\n",
       "<p data-translate=\"blocked_why_detail\">This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.</p>\n",
       "</div>\n",
       "<div class=\"cf-column\">\n",
       "<h2 data-translate=\"blocked_resolve_headline\">What can I do to resolve this?</h2>\n",
       "<p data-translate=\"blocked_resolve_detail\">You can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page.</p>\n",
       "</div>\n",
       "</div>\n",
       "</div><!-- /.section -->\n",
       "<div class=\"cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300\">\n",
       "<p class=\"text-13\">\n",
       "<span class=\"cf-footer-item sm:block sm:mb-1\">Cloudflare Ray ID: <strong class=\"font-semibold\">88f15f4df9f79f5e</strong></span>\n",
       "<span class=\"cf-footer-separator sm:hidden\">•</span>\n",
       "<span class=\"cf-footer-item hidden sm:block sm:mb-1\" id=\"cf-footer-item-ip\">\n",
       "      Your IP:\n",
       "      <button class=\"cf-footer-ip-reveal-btn\" id=\"cf-footer-ip-reveal\" type=\"button\">Click to reveal</button>\n",
       "<span class=\"hidden\" id=\"cf-footer-ip\">196.47.128.177</span>\n",
       "<span class=\"cf-footer-separator sm:hidden\">•</span>\n",
       "</span>\n",
       "<span class=\"cf-footer-item sm:block sm:mb-1\"><span>Performance &amp; security by</span> <a href=\"https://www.cloudflare.com/5xx-error-landing\" id=\"brand_link\" rel=\"noopener noreferrer\" target=\"_blank\">Cloudflare</a></span>\n",
       "</p>\n",
       "<script>(function(){function d(){var b=a.getElementById(\"cf-footer-item-ip\"),c=a.getElementById(\"cf-footer-ip-reveal\");b&&\"classList\"in b&&(b.classList.remove(\"hidden\"),c.addEventListener(\"click\",function(){c.classList.add(\"hidden\");a.getElementById(\"cf-footer-ip\").classList.remove(\"hidden\")}))}var a=document;document.addEventListener&&a.addEventListener(\"DOMContentLoaded\",d)})();</script>\n",
       "</div><!-- /.error-footer -->\n",
       "</div><!-- /#cf-error-details -->\n",
       "</div><!-- /#cf-wrapper -->\n",
       "<script>\n",
       "  window._cf_translation = {};\n",
       "  \n",
       "  \n",
       "</script>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# driver nodes/main function \n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\t# Data for URL \n",
    "\tjob = \"data+science+internship\"\n",
    "\tLocation = \"Noida%2C+Uttar+Pradesh\"\n",
    "\turl = \"https://in.indeed.com/jobs?q=\"+job+\"&l=\"+Location \n",
    "\n",
    "\t# Pass this URL into the soup \n",
    "\t# which will return \n",
    "\t# html string \n",
    "\tsoup = html_code(url) \n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a63ef3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da = soup.find_all(\"td\", class_=\"resultContent css-1qwrrf0 eu4oa1w0\")\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f09df0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\t# call job and company data \n",
    "\t# and store into it var \n",
    "\tjob_res = job_data(soup) \n",
    "    \n",
    "job_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e419f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
